{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Corpse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  9924k      0 --:--:-- --:--:-- --:--:-- 10.1M\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(\"\".join(vocab))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 54, 63, 58, 46, 53, 52]\n",
      "hii python\n"
     ]
    }
   ],
   "source": [
    "# Encode and Decode function\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: \"\".join(itos[i] for i in l)\n",
    "\n",
    "print(encode(\"hii python\"))\n",
    "print(decode(encode(\"hii python\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation data\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "When the inputs are: [24], target: 43\n",
      "When the inputs are: [24, 43], target: 58\n",
      "When the inputs are: [24, 43, 58], target: 5\n",
      "When the inputs are: [24, 43, 58, 5], target: 57\n",
      "When the inputs are: [24, 43, 58, 5, 57], target: 1\n",
      "When the inputs are: [24, 43, 58, 5, 57, 1], target: 46\n",
      "When the inputs are: [24, 43, 58, 5, 57, 1, 46], target: 43\n",
      "When the inputs are: [24, 43, 58, 5, 57, 1, 46, 43], target: 39\n",
      "When the inputs are: [44], target: 53\n",
      "When the inputs are: [44, 53], target: 56\n",
      "When the inputs are: [44, 53, 56], target: 1\n",
      "When the inputs are: [44, 53, 56, 1], target: 58\n",
      "When the inputs are: [44, 53, 56, 1, 58], target: 46\n",
      "When the inputs are: [44, 53, 56, 1, 58, 46], target: 39\n",
      "When the inputs are: [44, 53, 56, 1, 58, 46, 39], target: 58\n",
      "When the inputs are: [44, 53, 56, 1, 58, 46, 39, 58], target: 1\n",
      "When the inputs are: [52], target: 58\n",
      "When the inputs are: [52, 58], target: 1\n",
      "When the inputs are: [52, 58, 1], target: 58\n",
      "When the inputs are: [52, 58, 1, 58], target: 46\n",
      "When the inputs are: [52, 58, 1, 58, 46], target: 39\n",
      "When the inputs are: [52, 58, 1, 58, 46, 39], target: 58\n",
      "When the inputs are: [52, 58, 1, 58, 46, 39, 58], target: 1\n",
      "When the inputs are: [52, 58, 1, 58, 46, 39, 58, 1], target: 46\n",
      "When the inputs are: [25], target: 17\n",
      "When the inputs are: [25, 17], target: 27\n",
      "When the inputs are: [25, 17, 27], target: 10\n",
      "When the inputs are: [25, 17, 27, 10], target: 0\n",
      "When the inputs are: [25, 17, 27, 10, 0], target: 21\n",
      "When the inputs are: [25, 17, 27, 10, 0, 21], target: 1\n",
      "When the inputs are: [25, 17, 27, 10, 0, 21, 1], target: 54\n",
      "When the inputs are: [25, 17, 27, 10, 0, 21, 1, 54], target: 39\n"
     ]
    }
   ],
   "source": [
    "# Batches\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split: str):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    xb = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    yb = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return xb, yb\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When the inputs are: {context.tolist()}, target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BiGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.7051, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1744)\n",
      "\n",
      "pxMHoRFJa!JKmRjtXzfN:CERiC-KuDHoiMIB!o3QHN\n",
      ",SPyiFhRKuxZOMsB-ZJhsucL:wfzLSPyZalylgQUEU cLq,SqV&vW:hhi\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BiGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):  # idx is (B, T)\n",
    "        logits = self.token_embedding_table(idx)  # logits is (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # To fulfill PyTorch cross entropy requirements\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)  # PyTorch requires (B, C, T)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, num_examples):\n",
    "        for _ in range(num_examples):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]  # (B, C). Only needs the last result of block_size (T)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C). Softmax on dimension C\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BiGramLanguageModel(vocab_size=vocab_size)\n",
    "xb, yb = get_batch(\"train\")\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# We see the loss on the randomly inited bigram LM is close to uniformly distributed loss\n",
    "print(-torch.log(torch.tensor(1 / vocab_size)))\n",
    "\n",
    "# Inference\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), num_examples=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.658010721206665\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train(epoch: int):\n",
    "    for _ in range(epoch):\n",
    "        xb, yb = get_batch(\"train\")\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "print(train(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AU,\n",
      "Gowist Weano th t?EX&jFumat the of laticond ionedrt, cIce ce n thive'dZKIve, avind n, shadYZTH!abIJ&--cothere, m;\n",
      "Tr myoman iss no t Fk cois The fy.Ebe hysoimay atode isevV:ZUAElanon ishuromV&Qxl?ughen,\n",
      "\n",
      "TRT:\n",
      "Pl, nopomngoreppurkRCKpo, me s ft he te s I'tun tav&\n",
      "Bnd or o ft ges, IN.\n",
      "\n",
      "WARSio. IVT:\n",
      "SSivk!\n",
      "Hand, fatrarcugeQG, he ame s mymenosos and DKpue whea abvpangmymy dvin\n",
      "CJins wW:CJm bYWCkSise mye adettatlyxMOLWAscotheirilknes thethin w,S$\n",
      "\n",
      "IUzord;Wthimy my by ithes etavery y: hal; t d.\n",
      "NYw\n"
     ]
    }
   ],
   "source": [
    "# We see the pattern looks similar to Shakespeare\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), num_examples=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Attention Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a tensor (4, 8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute the cummulative average along the T-dimension.\n",
    "\n",
    "> (Causal Attention) We want the element to have information from elements that are in front of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the the result to the the same size as the original tensor: (4, 8, 2),\n",
    "\n",
    "where each element xbow[b_i, t_j, c_k] is avg(x[b_i, t_0 : t_j,c_k]), the average along the T-dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# Naive way\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "xbow = torch.zeros([B, T, C])  # Bad-Of-Words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, : t + 1]  # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)  # (C,)\n",
    "\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's leverage matrix multiplication to do it efficiently, without for-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 3.],\n",
      "        [3., 3.],\n",
      "        [7., 0.]])\n",
      "tensor([[12.,  6.],\n",
      "        [12.,  6.],\n",
      "        [12.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "# If we have a matrix all of 1s.\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "mat_A = torch.ones((3, 3))\n",
    "mat_B = torch.randint(0, 9, (3, 2), dtype=torch.float32)\n",
    "print(mat_A)\n",
    "print(mat_B)\n",
    "\n",
    "# The matmul just sums up all row in mat_B!\n",
    "print(mat_A @ mat_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 3.],\n",
      "        [3., 3.],\n",
      "        [7., 0.]])\n",
      "tensor([[ 2.,  3.],\n",
      "        [ 5.,  6.],\n",
      "        [12.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "# What if we only want to sum up the elements before self (inclusive)?\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Use lower triangular matrix (mask)!\n",
    "mat_A = torch.tril(torch.ones((3, 3)))\n",
    "mat_B = torch.randint(0, 9, (3, 2), dtype=torch.float32)\n",
    "print(mat_A)\n",
    "print(mat_B)\n",
    "\n",
    "# The matmul now sums up rows until the current row in mat_B.\n",
    "print(mat_A @ mat_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 3.],\n",
      "        [3., 3.],\n",
      "        [7., 0.]])\n",
      "tensor([[2.0000, 3.0000],\n",
      "        [2.5000, 3.0000],\n",
      "        [4.0000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "# To have the average, we just need to normalize the triangular matrix.\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "mat_A = torch.tril(torch.ones((3, 3)))\n",
    "mat_A /= mat_A.sum(dim=1, keepdim=True)\n",
    "mat_B = torch.randint(0, 9, (3, 2), dtype=torch.float32)\n",
    "print(mat_A)\n",
    "print(mat_B)\n",
    "\n",
    "# The matmul now averages up rows until the current row in mat_B.\n",
    "print(mat_A @ mat_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "wei = torch.tril(torch.ones((T, T)))\n",
    "wei /= wei.sum(dim=1, keepdim=True)\n",
    "print(wei)\n",
    "\n",
    "xbow2 = wei @ x\n",
    "assert torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use softmax to replace the manual normalization of the trilangular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that softmax firstly does `exp()`, thus we need to do `log()` to our triangular matrix.\n",
    "\n",
    "1s becomes 0s, and 0s becomes '-inf'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "print(wei)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow3, xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q, K, V\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# ==== single head self-attention ====\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "# matmul\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "# mask\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "# softmax\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# out = wei @ x\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Attention is a way of communication. The token at its current place can communicate with other tokens before or after it.\n",
    "\n",
    "`q` is the query, `k` is the key, `v` is the value.\n",
    "\n",
    "Every elment is asking for something using the query, and gives key as what it has. Once it gets attention from others, it gives value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. There is no information of space, or position in attention. Thus one needs to embed the position information before doing attention. In contract, CNN is space aware by nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. In the example above we only attend to the tokens that are in front, that is \"causal attention\", used in decoder, meaning that current token does not talk to future tokens. In general, there is no restriction and an token can attend to tokens that are both in front or behind (encoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Tokens only communicate within others in the same batch. There will not be communication / attention among tokens from different batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. In the example above we have q, k, v all from tokens themselves, i.e. self-attention. In some cases, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Scale. A coefficient of `head_size ** -0.5` is required to ensure variance close to 1 => no element is to picky after softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "k = torch.randn((B, T, head_size))\n",
    "q = torch.randn((B, T, head_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1016)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variance is close to 1\n",
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0290)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variance is close to 1\n",
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.8750)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No scaling -> variance is big\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0547)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After scaling, the variance is close to 1.\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1961, 0.2167, 0.2646, 0.1774, 0.1452])\n",
      "tensor([0.0413, 0.1122, 0.8292, 0.0152, 0.0021])\n"
     ]
    }
   ],
   "source": [
    "# wei will go through softmax, which amplifies the most significant element.\n",
    "print(F.softmax(torch.tensor([0.2, 0.3, 0.5, 0.1, -0.1]), dim=-1))\n",
    "print(F.softmax(torch.tensor([0.2, 0.3, 0.5, 0.1, -0.1]) * 10, dim=-1))\n",
    "\n",
    "# We don't want any single element to always have significant effect in attention, especially in initialization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2241e891bdcb43d70b129352065e4a3e3c43dbe26992820aebe42833f1782192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
