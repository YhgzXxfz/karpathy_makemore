{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = 2 * a\n",
    "\n",
    "db = torch.tensor(0.5)\n",
    "da = 2.0 * db\n",
    "b.backward(gradient=db)\n",
    "print(da)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(alphabet)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "import typing as tp\n",
    "\n",
    "\n",
    "def build_dataset(words: tp.List):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        window = [0] * 3\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(window)\n",
    "            Y.append(ix)\n",
    "            window = window[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "train_size = int(0.8 * len(words))\n",
    "validation_size = int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:train_size])\n",
    "Xval, Yval = build_dataset(words[train_size:validation_size])\n",
    "Xtest, Ytest = build_dataset(words[validation_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def cmp(s: str, derivative: torch.Tensor, tensor: torch.Tensor) -> None:\n",
    "    ex = torch.all(derivative == tensor.grad).item()\n",
    "    app = torch.allclose(derivative, tensor.grad)\n",
    "    maxdiff = (derivative - tensor.grad).abs().max().item()\n",
    "    print(f\"{s:5s} | Exact: {str(ex):5s} | Approximate: {str(app):5s} | MaxDiff: {str(maxdiff):5s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n"
     ]
    }
   ],
   "source": [
    "cmp(\"test\", da, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ALPHABET_SPACE = len(stoi)\n",
    "NGRAM_SIZE = 3\n",
    "REPRESENTATION_DIM = 10\n",
    "HIDDEN_LAYER_DIM = 64\n",
    "MINI_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "import torch\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn([ALPHABET_SPACE, REPRESENTATION_DIM], generator=g)  # (27, 10)\n",
    "# Layer 1\n",
    "W1 = (\n",
    "    torch.randn([NGRAM_SIZE * REPRESENTATION_DIM, HIDDEN_LAYER_DIM], generator=g)\n",
    "    * (5 / 3)\n",
    "    / ((NGRAM_SIZE * REPRESENTATION_DIM) ** 0.5)  # Kaiming's Init\n",
    ")  # (30, 64)\n",
    "b1 = torch.randn([HIDDEN_LAYER_DIM], generator=g) * 0.1  # (64,)\n",
    "# Layer 2\n",
    "W2 = torch.randn([HIDDEN_LAYER_DIM, ALPHABET_SPACE], generator=g) * 0.1  # (64, 27)\n",
    "b2 = torch.randn([ALPHABET_SPACE], generator=g) * 0.1  # (27,)\n",
    "# BatchNorm Layer\n",
    "bngain = torch.randn([1, HIDDEN_LAYER_DIM]) * 0.1 + 1.0  # (64,)\n",
    "bnbias = torch.randn([1, HIDDEN_LAYER_DIM]) * 0.1  # (64,)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One mini batch\n",
    "ix = torch.randint(0, Xtr.shape[0], (MINI_BATCH_SIZE,), generator=g)\n",
    "\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3499, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed Forward Once\n",
    "\"\"\"\n",
    "emb = C[Xb]\n",
    "h = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\"\"\"\n",
    "emb = C[Xb]\n",
    "emb_concat = emb.view(emb.shape[0], -1)\n",
    "# Linear Layer 1\n",
    "linear = emb_concat @ W1 + b1\n",
    "# Batch Norm\n",
    "bn_mean = (1 / MINI_BATCH_SIZE) * linear.sum(dim=0, keepdim=True)\n",
    "bn_diff = linear - bn_mean\n",
    "bn_diff_square = bn_diff**2\n",
    "bn_var = (1 / (MINI_BATCH_SIZE - 1)) * bn_diff_square.sum(dim=0, keepdim=True)\n",
    "bn_var_inv = (bn_var + 1e-5) ** -0.5\n",
    "bn_raw = bn_diff * bn_var_inv\n",
    "bn_scaled = bngain * bn_raw + bnbias\n",
    "# Activation\n",
    "hidden = torch.tanh(bn_scaled)\n",
    "# Linear Layer 2\n",
    "logits = hidden @ W2 + b2\n",
    "# Cross Entropy\n",
    "logits_max = logits.max(\n",
    "    1, keepdim=True\n",
    ").values  # For numerical stability. Softmax(x_i) = (exp(x_i - max(x_i)) / sum(exp(x_i - max(x_i))))\n",
    "logits_norm = logits - logits_max\n",
    "counts = logits_norm.exp()\n",
    "counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(MINI_BATCH_SIZE), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [\n",
    "    logprobs,\n",
    "    probs,\n",
    "    counts_sum_inv,\n",
    "    counts_sum,\n",
    "    counts,\n",
    "    logits_norm,\n",
    "    logits_max,\n",
    "    logits,\n",
    "    hidden,\n",
    "    bn_scaled,\n",
    "    bn_raw,\n",
    "    bn_var_inv,\n",
    "    bn_var,\n",
    "    bn_diff_square,\n",
    "    bn_diff,\n",
    "    bn_mean,\n",
    "    linear,\n",
    "    emb_concat,\n",
    "    emb,\n",
    "]:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogprobs | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dprobs | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dcounts_sum_inv | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dcounts_sum | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dcounts | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dlogits_norm | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dlogits_max | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dlogits | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dhidden | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dW2   | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "db2   | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dbn_scaled | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dbngain | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dbnbias | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dbn_raw | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dbn_var_inv | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dbn_var | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dbn_diff_square | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dbn_diff | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dbn_mean | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dlinear | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "demb_concat | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dW1   | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "db1   | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "demb  | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n",
      "dC    | Exact: True  | Approximate: True  | MaxDiff: 0.0  \n"
     ]
    }
   ],
   "source": [
    "# Backward Pass\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ======== Cross Entropy ========\n",
    "# loss = -logprobs[range(MINI_BATCH_SIZE), Yb].mean()\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(logprobs.shape[0]), Yb] = -1.0 / logprobs.shape[0]\n",
    "cmp(\"dlogprobs\", dlogprobs, logprobs)\n",
    "\n",
    "# logprobs = probs.log()\n",
    "dprobs = (probs**-1) * dlogprobs\n",
    "cmp(\"dprobs\", dprobs, probs)\n",
    "\n",
    "# probs = counts * counts_sum_inv\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True)\n",
    "cmp(\"dcounts_sum_inv\", dcounts_sum_inv, counts_sum_inv)\n",
    "dcounts_1 = counts_sum_inv * dprobs\n",
    "\n",
    "# counts_sum_inv = counts_sum ** -1\n",
    "dcounts_sum = (-1.0) * (counts_sum**-2) * dcounts_sum_inv\n",
    "cmp(\"dcounts_sum\", dcounts_sum, counts_sum)\n",
    "\n",
    "# counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "dcounts_2 = torch.ones_like(counts) * dcounts_sum\n",
    "dcounts = dcounts_1 + dcounts_2\n",
    "cmp(\"dcounts\", dcounts, counts)\n",
    "\n",
    "# counts = logits_norm.exp()\n",
    "dlogits_norm = counts * dcounts  # logits_norm.exp() * dcounts == counts * dcounts\n",
    "cmp(\"dlogits_norm\", dlogits_norm, logits_norm)\n",
    "\n",
    "# logits_norm = logits - logits_max\n",
    "dlogits_max = torch.sum(-1.0 * torch.ones_like(logits_norm) * dlogits_norm, dim=1, keepdim=True)\n",
    "cmp(\"dlogits_max\", dlogits_max, logits_max)\n",
    "\n",
    "# logits_max = logits.max(1, keepdim=True).values\n",
    "dlogits = (\n",
    "    torch.ones_like(logits) * dlogits_norm\n",
    "    + F.one_hot(logits.max(dim=1, keepdim=False).indices, num_classes=logits.shape[1]) * dlogits_max\n",
    ")\n",
    "cmp(\"dlogits\", dlogits, logits)\n",
    "\n",
    "# ======== Layer 2 =========\n",
    "# logits = hidden @ W2 + b2\n",
    "dhidden = dlogits @ W2.T\n",
    "cmp(\"dhidden\", dhidden, hidden)\n",
    "dW2 = hidden.T @ dlogits\n",
    "cmp(\"dW2\", dW2, W2)\n",
    "db2 = torch.sum(dlogits, dim=0, keepdim=False)\n",
    "cmp(\"db2\", db2, b2)\n",
    "\n",
    "# ========= Activation ========\n",
    "# hidden = torch.tanh(bn_scaled)\n",
    "dbn_scaled = (1.0 - hidden**2) * dhidden\n",
    "cmp(\"dbn_scaled\", dbn_scaled, bn_scaled)\n",
    "\n",
    "# ========= Batch Norm ========\n",
    "# bn_scaled = bngain * bn_raw + bnbias\n",
    "dbngain = (bn_raw * dbn_scaled).sum(dim=0, keepdim=True)\n",
    "cmp(\"dbngain\", dbngain, bngain)\n",
    "dbnbias = torch.sum(torch.ones_like(dbn_scaled) * dbn_scaled, dim=0, keepdim=True)\n",
    "cmp(\"dbnbias\", dbnbias, bnbias)\n",
    "dbn_raw = bngain * dbn_scaled\n",
    "cmp(\"dbn_raw\", dbn_raw, bn_raw)\n",
    "\n",
    "# bn_raw = bn_diff * bn_var_inv\n",
    "dbn_var_inv = (bn_diff * dbn_raw).sum(dim=0, keepdim=True)\n",
    "cmp(\"dbn_var_inv\", dbn_var_inv, bn_var_inv)\n",
    "dbn_diff_1 = bn_var_inv * dbn_raw\n",
    "\n",
    "# bn_var_inv = (bn_var + 1e-5) ** -0.5\n",
    "dbn_var = (-0.5 * (bn_var + 1e-5) ** -1.5) * dbn_var_inv\n",
    "cmp(\"dbn_var\", dbn_var, bn_var)\n",
    "\n",
    "# bn_var = (1/(MINI_BATCH_SIZE-1)) * bn_diff_square.sum(dim=0, keepdim=True)\n",
    "dbn_diff_square = (1 / (MINI_BATCH_SIZE - 1)) * torch.ones_like(bn_diff_square) * dbn_var\n",
    "cmp(\"dbn_diff_square\", dbn_diff_square, bn_diff_square)\n",
    "\n",
    "# bn_diff_square = bn_diff ** 2\n",
    "dbn_diff_2 = 2.0 * bn_diff * dbn_diff_square\n",
    "dbn_diff = dbn_diff_1 + dbn_diff_2\n",
    "cmp(\"dbn_diff\", dbn_diff, bn_diff)\n",
    "\n",
    "# bn_diff = linear - bn_mean\n",
    "dbn_mean = torch.sum(-1.0 * torch.ones_like(bn_diff) * dbn_diff, dim=0, keepdim=True)\n",
    "cmp(\"dbn_mean\", dbn_mean, bn_mean)\n",
    "# bn_mean = (1/MINI_BATCH_SIZE) * linear.sum(dim=0, keepdim=True)\n",
    "dlinear = torch.ones_like(linear) * dbn_diff + (1 / MINI_BATCH_SIZE) * torch.ones_like(linear) * dbn_mean\n",
    "cmp(\"dlinear\", dlinear, linear)\n",
    "\n",
    "# ======== Layer 1 =========\n",
    "# linear = emb_concat @ W1 + b1\n",
    "demb_concat = dlinear @ W1.T\n",
    "cmp(\"demb_concat\", demb_concat, emb_concat)\n",
    "dW1 = emb_concat.T @ dlinear\n",
    "cmp(\"dW1\", dW1, W1)\n",
    "db1 = dlinear.sum(dim=0, keepdim=True)\n",
    "cmp(\"db1\", db1, b1)\n",
    "\n",
    "\n",
    "# ========= Embedding =========\n",
    "# emb_concat = emb.view(emb.shape[0], -1)\n",
    "demb = demb_concat.view(emb.shape)\n",
    "cmp(\"demb\", demb, emb)\n",
    "\n",
    "# emb = C[Xb]\n",
    "# emb = F.one_hot(Xb, num_classes=C.shape[0]) @ C\n",
    "dC = torch.zeros_like(C)\n",
    "for row in range(Xb.shape[0]):\n",
    "    for col in range(Xb.shape[1]):\n",
    "        ix = Xb[row][col]\n",
    "        dC[ix] += demb[row][col]\n",
    "\n",
    "cmp(\"dC\", dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Backprop for the Loss function (Cross Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.349898099899292 diff: -2.384185791015625e-07\n",
      "dlogits | Exact: False | Approximate: True  | MaxDiff: 6.05359673500061e-09\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Cross Entropy\n",
    "logits_max = logits.max(1, keepdim=True).values\n",
    "logits_norm = logits - logits_max\n",
    "counts = logits_norm.exp()\n",
    "counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "counts_sum_inv = counts_sum ** -1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(MINI_BATCH_SIZE), Yb].mean()\n",
    "\"\"\"\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), \"diff:\", (loss_fast - loss).item())\n",
    "\n",
    "# Backward Pass\n",
    "dlogits = F.softmax(logits, dim=1)\n",
    "dlogits[range(MINI_BATCH_SIZE), Yb] -= 1\n",
    "dlogits /= MINI_BATCH_SIZE\n",
    "cmp(\"dlogits\", dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we want the output of the softmax layer (`F.softmax(logits, dim=1)`) to be all 0, except the one that matches the label in the target to be 1. If that was the case, then dlogits would be all 0 and the model is perfect.\n",
    "\n",
    "In reality, the derivative substracts 1 from the value of the correct index in softmax outputs, making it as negative as possible. This actually boosts its value to be as positive towards 1 as possible through optimizer (v.data += -lr * v.grad). Whereas for other indexes, the process decreases their values to be as close to 0 as possible given their gradients are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st row of Softmax: tensor([0.0646, 0.0904, 0.0189, 0.0523, 0.0193, 0.0812, 0.0267, 0.0355, 0.0179,\n",
      "        0.0311, 0.0359, 0.0355, 0.0388, 0.0298, 0.0357, 0.0141, 0.0100, 0.0198,\n",
      "        0.0172, 0.0516, 0.0475, 0.0216, 0.0276, 0.0675, 0.0620, 0.0271, 0.0206],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "selected 8th element\n",
      "Sum of 1st row: -5.21540641784668e-08\n",
      "1st row: tensor([ 0.0646,  0.0904,  0.0189,  0.0523,  0.0193,  0.0812,  0.0267,  0.0355,\n",
      "        -0.9821,  0.0311,  0.0359,  0.0355,  0.0388,  0.0298,  0.0357,  0.0141,\n",
      "         0.0100,  0.0198,  0.0172,  0.0516,  0.0475,  0.0216,  0.0276,  0.0675,\n",
      "         0.0620,  0.0271,  0.0206], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15a6edc30>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"1st row of Softmax: {F.softmax(logits, dim=1)[0]}\")\n",
    "print(f\"selected {Yb[0]}th element\")\n",
    "# Each row of dlogits sums to 0.\n",
    "# Because d_cross_entropy = Softmax - one_hot, whereas the sum of softmax equals to 1, and sum of one_hot also equals to 1.\n",
    "print(f\"Sum of 1st row: {dlogits[0].sum() * MINI_BATCH_SIZE}\")\n",
    "# The one selected (10th) has the negative gradients, kind of pulling down the plane.\n",
    "print(f\"1st row: {dlogits[0] * MINI_BATCH_SIZE}\")\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits.detach(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast backprop for the BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(8.9874, grad_fn=<MaxBackward1>)\n",
      "dlinear | Exact: False | Approximate: True  | MaxDiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "bn_mean = (1/MINI_BATCH_SIZE) * linear.sum(dim=0, keepdim=True)\n",
    "bn_diff = linear - bn_mean\n",
    "bn_diff_square = bn_diff ** 2\n",
    "bn_var = (1/(MINI_BATCH_SIZE-1)) * bn_diff_square.sum(dim=0, keepdim=True)\n",
    "bn_var_inv = (bn_var + 1e-5) ** -0.5\n",
    "bn_raw = bn_diff * bn_var_inv\n",
    "bn_scaled = bngain * bn_raw + bnbias\n",
    "\"\"\"\n",
    "bn_scaled_fast = (\n",
    "    bngain * (linear - linear.mean(0, keepdim=True)) / (linear.var(0, keepdim=True, unbiased=True) + 1e-5) ** -0.5\n",
    "    + bnbias\n",
    ")\n",
    "print(\"max diff:\", (bn_scaled_fast - bn_scaled).abs().max())\n",
    "\n",
    "# Backprop\n",
    "dlinear = (\n",
    "    bngain\n",
    "    * bn_var_inv\n",
    "    / MINI_BATCH_SIZE\n",
    "    * (\n",
    "        MINI_BATCH_SIZE * dbn_scaled\n",
    "        - dbn_scaled.sum(0)\n",
    "        - MINI_BATCH_SIZE / (MINI_BATCH_SIZE - 1) * bn_raw * (dbn_scaled * bn_raw).sum(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "cmp(\"dlinear\", dlinear, linear)  # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n",
      "      0/ 200000: 3.3378\n",
      "  10000/ 200000: 2.4359\n",
      "  20000/ 200000: 2.1127\n",
      "  30000/ 200000: 2.3243\n",
      "  40000/ 200000: 2.2405\n",
      "  50000/ 200000: 2.1444\n",
      "  60000/ 200000: 2.1594\n",
      "  70000/ 200000: 2.3827\n",
      "  80000/ 200000: 2.1310\n",
      "  90000/ 200000: 2.5554\n",
      " 100000/ 200000: 2.1805\n",
      " 110000/ 200000: 1.8397\n",
      " 120000/ 200000: 2.0938\n",
      " 130000/ 200000: 2.5371\n",
      " 140000/ 200000: 1.8366\n",
      " 150000/ 200000: 2.1594\n",
      " 160000/ 200000: 2.0352\n",
      " 170000/ 200000: 2.1917\n",
      " 180000/ 200000: 1.9685\n",
      " 190000/ 200000: 1.9738\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "ALPHABET_SPACE = len(stoi)\n",
    "NGRAM_SIZE = 3\n",
    "REPRESENTATION_DIM = 10\n",
    "HIDDEN_LAYER_DIM = 64\n",
    "MINI_BATCH_SIZE = 32\n",
    "\n",
    "# Initialization\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn([ALPHABET_SPACE, REPRESENTATION_DIM], generator=g)  # (27, 10)\n",
    "# Layer 1\n",
    "W1 = (\n",
    "    torch.randn([NGRAM_SIZE * REPRESENTATION_DIM, HIDDEN_LAYER_DIM], generator=g)\n",
    "    * (5 / 3)\n",
    "    / ((NGRAM_SIZE * REPRESENTATION_DIM) ** 0.5)\n",
    ")  # (30, 64)\n",
    "b1 = torch.randn([HIDDEN_LAYER_DIM], generator=g) * 0.1  # (64,)\n",
    "# Layer 2\n",
    "W2 = torch.randn([HIDDEN_LAYER_DIM, ALPHABET_SPACE], generator=g) * 0.1  # (64, 27)\n",
    "b2 = torch.randn([ALPHABET_SPACE], generator=g) * 0.1  # (27,)\n",
    "# BatchNorm Layer\n",
    "bngain = torch.randn([1, HIDDEN_LAYER_DIM]) * 0.1 + 1.0  # (64,)\n",
    "bnbias = torch.randn([1, HIDDEN_LAYER_DIM]) * 0.1  # (64,)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "#\n",
    "max_steps = 200000\n",
    "lossi = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(max_steps):\n",
    "        # mini batch\n",
    "        ix = torch.randint(0, Xtr.shape[0], (MINI_BATCH_SIZE,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "        # forward\n",
    "        emb = C[Xb]\n",
    "        emb_concat = emb.view(Xb.shape[0], -1)\n",
    "\n",
    "        linear = emb_concat @ W1 + b1\n",
    "\n",
    "        bn_mean = linear.mean(dim=0, keepdim=True)\n",
    "        bn_var = linear.var(dim=0, unbiased=True, keepdim=True)\n",
    "        bn_var_inv = (bn_var + 1e-5) ** -0.5\n",
    "        bn_raw = (linear - bn_mean) * bn_var_inv\n",
    "        bn_scaled = bngain * bn_raw + bnbias\n",
    "\n",
    "        hidden = torch.tanh(bn_scaled)\n",
    "\n",
    "        logits = hidden @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "        # Backprop\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "\n",
    "        # PyTorch backprop\n",
    "        # loss.backward()\n",
    "\n",
    "        # Manual backprop\n",
    "        dlogits = F.softmax(logits, dim=1)\n",
    "        dlogits[range(MINI_BATCH_SIZE), Yb] -= 1\n",
    "        dlogits /= MINI_BATCH_SIZE\n",
    "\n",
    "        dhidden = dlogits @ W2.T\n",
    "        dW2 = hidden.T @ dlogits\n",
    "        db2 = dlogits.sum(dim=0, keepdim=False)\n",
    "\n",
    "        dbn_scaled = (1.0 - hidden**2) * dhidden\n",
    "\n",
    "        dbngain = (bn_raw * dbn_scaled).sum(dim=0, keepdim=True)\n",
    "        dbnbias = dbn_scaled.sum(dim=0, keepdim=True)\n",
    "        dlinear = (\n",
    "            bngain\n",
    "            * bn_var_inv\n",
    "            / MINI_BATCH_SIZE\n",
    "            * (\n",
    "                MINI_BATCH_SIZE * dbn_scaled\n",
    "                - dbn_scaled.sum(0)\n",
    "                - MINI_BATCH_SIZE / (MINI_BATCH_SIZE - 1) * bn_raw * (dbn_scaled * bn_raw).sum(0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        demb_concat = dlinear @ W1.T\n",
    "        dW1 = emb_concat.T @ dlinear\n",
    "        db1 = dlinear.sum(dim=0, keepdim=False)\n",
    "\n",
    "        demb = demb_concat.view(emb.shape)\n",
    "\n",
    "        dC = torch.zeros_like(C)\n",
    "        for row in range(Xb.shape[0]):\n",
    "            for col in range(Xb.shape[1]):\n",
    "                ix = Xb[row][col]\n",
    "                dC[ix] += demb[row][col]\n",
    "\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "        # Optimize\n",
    "        lr = 0.1 if i < 100_000 else 0.01\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            # p.data += lr * p.grad # PyTorch\n",
    "            p.data += -lr * grad\n",
    "\n",
    "        if i % 10_000 == 0:\n",
    "            print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "        lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate after training\n",
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    emb_concat = emb.view(Xtr.shape[0], -1)\n",
    "    linear = emb_concat @ W1 + b1\n",
    "    bn_mean = linear.mean(dim=0, keepdim=True)\n",
    "    bn_var = linear.var(dim=0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1463329792022705\n",
      "val 2.162074565887451\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split: str):\n",
    "    x, y = {\"train\": (Xtr, Ytr), \"val\": (Xval, Yval), \"test\": (Xtest, Ytest)}[split]\n",
    "    emb = C[x]\n",
    "    emb_concat = emb.view(x.shape[0], -1)\n",
    "    linear = emb_concat @ W1 + b1\n",
    "    bn_scaled = bngain * (linear - bn_mean) * (bn_var + 1e-5) ** -0.5 + bnbias\n",
    "    hidden = torch.tanh(bn_scaled)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah.\n",
      "amelle.\n",
      "khi.\n",
      "mrixhelty.\n",
      "salaysie.\n",
      "rahnen.\n",
      "den.\n",
      "rha.\n",
      "kaeli.\n",
      "nellaiah.\n",
      "maiivon.\n",
      "leigh.\n",
      "ham.\n",
      "joce.\n",
      "quinn.\n",
      "shon.\n",
      "marianni.\n",
      "wanthoniearisi.\n",
      "jacee.\n",
      "durli.\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    outs = []\n",
    "    context = [0] * NGRAM_SIZE\n",
    "\n",
    "    while True:\n",
    "        # Forward pass:\n",
    "        # Embedding:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        emb_concat = emb.view(emb.shape[0], -1)\n",
    "        linear = emb_concat @ W1 + b1\n",
    "        bn_scaled = bngain * (linear - bn_mean) * (bn_var + 1e-5) ** -0.5 + bnbias\n",
    "        hidden = torch.tanh(bn_scaled)\n",
    "        logits = hidden @ W2 + b2\n",
    "\n",
    "        # Sampling\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        outs.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in outs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7dc663557200197b1ecb639904d6775340f1f12cbb992d7264be98ae917f5fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
